---
title: "Lab 7: Machine Learning"
author: "Adam Bisharat"
format: gfm
---

Today we are going to learn how to apply different machine learning methods, beginning with clustering: 

The goal here is to find groups/clusters in your input data

First I will make up some data with clear groups. For this I will use the `rnorm()` function: 
```{r}
rnorm(10)
```

```{r}
hist(rnorm(10000,3))
```


```{r}
hist(c(rnorm(10000, -3), rnorm(10000,3)))
```
```{r}
n <- 10000
x <- c(rnorm(n,-3), rnorm(n,3))
hist(x)
```

```{r}
n <- 30
x <- c(rnorm(n,-3), rnorm(n,3))
hist(x)
```

```{r}
n <- 30
x <- c(rnorm(n,-3), rnorm(n,3))
y <- rev(x)

z <- cbind(x,y)
head(z)
```

```{r}
plot(z)
```

Use the `kmeans()` function setting k to 2 and nstart=20

Inspect/print the results

> Q. How many points are in each cluster?

30 points in each cluster, 60 total

> Q. What ‘component’ of your result object details
 - cluster size?
 - cluster assignment/membership?
 - cluster center?
 
> Q. Plot x colored by the kmeans cluster assignment and
 add cluster centers as blue points
 
```{r}
km <- kmeans(z, centers=2)
km
```

Results in kmeans objects `km`
```{r}
attributes(km)
```
 Cluster size?  
```{r}
km$size
```
 
Cluster Assignment/membership?
```{r}
km$cluster
```
 
Cluster center?
```{r}
km$center
```

Plot x colored by the kmeans cluster assignment and
 add cluster centers as blue points
```{r}
plot(z, col="red")
```
R will recycle the shorter color vector to be the same length as the longer(number of data points) in z 
```{r}
plot(z, col=c("red", "blue") )
```
 
```{r}
plot(z, col=c( 1,2) )
```

```{r}
plot(z, col=km$cluster)
```

We can use the `points()` fucntion to add new points to an existing plot...like the cluster centers. 
```{r}
plot(z, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=3)
```

> Q. Can you run kmeans() and ask for 4 clusters please and plot the results like we have done above?

```{r}
km4 <- kmeans(z, centers=4)
plot(z, col=km$cluster)
points(km4$centers, col="blue", pch=15, cex=1.5)
```



Hierarchal clustering:
lets take our same data 'z' and see how hclust() works


First we need a distance matrix of our data to be clustered
```{r}
d <- dist(z)
hc <- hclust(d)
hc
```


```{r}
plot(hc)
abline(h=8, col="red")
```
One branch side of the plot is all 1-30 while the other is 31-60

I can get my cluster membership vector by "cutting the tree" with the 'cutree()' function like so:


```{r}
grps <- cutree(hc, h=8)
grps
```

Q. Can you plot 'z' colored by our hclust results:

```{r}
plot(z, col=grps)
```


PCA: Principle Component Analysis



START OF LAB MATERIALS

Importing Data
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
head(x)
```

Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x)
```

Using the Dim() function we found that there are 17 rows and 4 columns


Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

I prefer the second method. The second method allows me to edit row names in a much more direct, and in my opinion, clean fashion than the first method which requires a whole new line of code.


```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

Q3: Changing what optional argument in the above barplot() function results in the following plot?

Changing the beside function to false will yield the plot as seen below.

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```


Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

The code pairs(x, col=rainbow(10), pch=16) generates pairwise scatter plots for all combinations of variables in x, using a rainbow color scheme and filled circle points. If a point lies on the diagonal in a given plot, it indicates that the values for the two variables being compared are equal at that point.

Class note: pairs plots can be useful for small sets of data but for large sets of data its ridiculous to be expected to see trens using this plot. PCA will help.



```{r}
pairs(x, col=rainbow(10), pch=16)
```

Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

Northern Ireland consumes much more fresh potatos and less fresh fruit.


How does PCA capture this data?

```{r}
pca <- prcomp (t(x))
summary(pca)

```

```{r}
pca$x
```


Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

The completed code is below 
```{r}
plot(pca$x[,1], pca$x[,2],
 
     xlab="PC1 (67.4%)", ylab="PC2 (29%)", xlim=c(-270,500))

#Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document. 

text(pca$x[,1], pca$x[,2], colnames(x),col=c("red", "orange", "blue", "green"))
```
    
     col=c("red", "black", "blue", "green"), pch=16,
     
```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

```{r}
z <- summary(pca)
z$importance
```

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )

```
    

Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

The two food groups that feature prominently are Fresh Potatoes and Soft Drinks. PC2 mainly tells us about the second greatest amount of variation between the countries.

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )

```


